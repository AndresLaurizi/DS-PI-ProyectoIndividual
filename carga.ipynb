{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#localidades = pd.read_csv(\"DS-PI-ProyectoIndividual\\Datasets\\Localidades.csv\",delimiter = ',',encoding = \"utf-8\")\n",
    "#clientes = pd.read_csv(\"DS-PI-ProyectoIndividual\\Datasets\\Clientes.csv\",delimiter = ';',encoding = \"utf-8\")\n",
    "#clientes = pd.read_csv(\"DS-PI-ProyectoIndividual\\Datasets\\Clientes.csv\",encoding = \"utf-8\",delimiter = ';', usecols=[\"ID\",\"Provincia\",\"Nombre_y_Apellido\",\"Domicilio\",\"Telefono\",\"Edad\",\"Localidad\",\"X\",\"Y\",\"col10\"])\n",
    "#proveedores = pd.read_csv(\"DS-PI-ProyectoIndividual\\Datasets\\Proveedores.csv\",delimiter = ',',encoding = \"ansi\")\n",
    "gasto = pd.read_csv(\"C:\\\\Users\\\\andre\\\\OneDrive\\\\Escritorio\\\\IMPORTANTES\\\\TP Individual N1\\\\DS-PI-ProyectoIndividual\\\\Datasets\\\\Gasto.csv\",delimiter = ',',encoding = \"utf-8\")\n",
    "#compra = pd.read_csv(\"DS-PI-ProyectoIndividual\\Datasets\\Compra.csv\",delimiter = ',',encoding = \"utf-8\")\n",
    "sucursales = pd.read_csv(\"C:\\\\Users\\\\andre\\\\OneDrive\\\\Escritorio\\\\IMPORTANTES\\\\TP Individual N1\\\\DS-PI-ProyectoIndividual\\\\Datasets\\\\Sucursales.csv\",delimiter = ';',encoding = \"utf-8\")\n",
    "#venta = pd.read_csv(\"DS-PI-ProyectoIndividual\\Datasets\\Venta.csv\",delimiter = ',',encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizacion outliers venta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargadeVentas():\n",
    "    #ds_path= pathlib.Path(\"Datasets\")\n",
    "    #print(ds_path)\n",
    "    df_list=[]\n",
    "    #archivo=glob.glob('.csv')\n",
    "    #print(\"Hola\")\n",
    "    for file in glob.glob('Vent*.csv'):\n",
    "        #print(file)\n",
    "        df=pd.read_csv(file,delimiter = ',',encoding = \"utf-8\")\n",
    "        df_list.append(df)\n",
    "    venta=pd.concat(df_list)\n",
    "    #Tratamiento de Outliers\n",
    "    Q1 = venta[\"Precio\"].quantile(0.25)\n",
    "    Q3 = venta[\"Precio\"].quantile(0.75)\n",
    "    #Se calcula el rango intercuartilico IQR.\n",
    "    IQR = Q3 -Q1\n",
    "    outliersSup = (Q3 + (1.5*IQR)) \n",
    "    mask = venta[\"Precio\"]<outliersSup\n",
    "    ventaSinOut2 = venta[mask]\n",
    "    #ventaSinOut2.describe().T\n",
    "    Precio_Producto= ventaSinOut2.groupby([\"IdProducto\"])[\"Precio\"].mean().reset_index()\n",
    "    Precio_Producto= Precio_Producto.rename(columns= {\"Precio\":\"Precio2\"})\n",
    "    venta2=venta.copy()\n",
    "    venta2 = pd.merge(venta2,Precio_Producto, how=\"left\", on=[\"IdProducto\"])\n",
    "    venta2.sort_values(\"IdVenta\")\n",
    "    #se reemplazan los 0 de precio2 por el valor de la media de los precios sin outliers.\n",
    "    venta2.Precio2.fillna(ventaSinOut2.Precio.mean(),inplace=True) #$1065\n",
    "\n",
    "    #se reemplazan los nan por 0 en la columna precio.\n",
    "    venta2.Precio.fillna(0,inplace=True)\n",
    "    #se insertan 1 cuando la cantidad es 0.\n",
    "    venta2.Cantidad.fillna(1,inplace=True)\n",
    "    #Se crea una columna con unos (1) \n",
    "\n",
    "    venta2[\"Outliers\"]=np.ones(venta2.shape[0])\n",
    "    #se marcan con 0 las filas que son outlaiers, para luego ser reemplazadas por Precio2\n",
    "    venta2.loc[(venta2['Precio'] >= outliersSup),\"Outliers\"] = 0\n",
    "    #Este bucle reemplaza los ceros y outliers por los valores promedios \n",
    "    for i in range(0,venta2.shape[0]):\n",
    "        if venta2.Precio[i] == 0 or venta2.Precio[i]>outliersSup:\n",
    "            venta2.iloc[i,8]=venta2.iloc[i,10] # 8 es precio, 10 es precio 2\n",
    "    #se elimina la columna Precio2\n",
    "    venta2.drop(columns=\"Precio2\", inplace=True)\n",
    "    #Se crea la columna Total\n",
    "    #venta2[\"Total\"]= venta2[\"Precio\"]*venta2[\"Cantidad\"]\n",
    "    #Tratamiento de Outliers columna cantidad\n",
    "    Q1 = venta2[\"Cantidad\"].quantile(0.25)\n",
    "    Q3 = venta2[\"Cantidad\"].quantile(0.75)\n",
    "    #Se calcula el rango intercuartilico IQR.\n",
    "    IQR = Q3 -Q1\n",
    "    outliersSup = (Q3 + (1.5*IQR)) \n",
    "    mask = venta2[\"Cantidad\"]<outliersSup\n",
    "    ventaCantSinOut = venta2[mask]\n",
    "    cantidadMedia = round(ventaCantSinOut.Cantidad.mean())\n",
    "    venta2.loc[(venta2['Cantidad'] >= outliersSup),\"Cantidad\"] = cantidadMedia #2\n",
    "    #Se crea la columna Total\n",
    "    venta2[\"Total\"]= venta2[\"Precio\"]*venta2[\"Cantidad\"]\n",
    "    venta2.to_csv (r'C:\\Users\\andre\\OneDrive\\Escritorio\\TP Individual N1\\DS-PI-ProyectoIndividual\\datasetsProcesados\\ventas_dataframe.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'C:\\Users\\andre\\OneDrive\\Escritorio\\TP Individual N1\\DS-PI-ProyectoIndividual\\datasetsProcesados'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\OneDrive\\Escritorio\\IMPORTANTES\\TP Individual N1\\carga.ipynb Celda 5\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Escritorio/IMPORTANTES/TP%20Individual%20N1/carga.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cargadeVentas()\n",
      "\u001b[1;32mc:\\Users\\andre\\OneDrive\\Escritorio\\IMPORTANTES\\TP Individual N1\\carga.ipynb Celda 5\u001b[0m in \u001b[0;36mcargadeVentas\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Escritorio/IMPORTANTES/TP%20Individual%20N1/carga.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m#Se crea la columna Total\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Escritorio/IMPORTANTES/TP%20Individual%20N1/carga.ipynb#W4sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m venta2[\u001b[39m\"\u001b[39m\u001b[39mTotal\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m=\u001b[39m venta2[\u001b[39m\"\u001b[39m\u001b[39mPrecio\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m*\u001b[39mventa2[\u001b[39m\"\u001b[39m\u001b[39mCantidad\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Escritorio/IMPORTANTES/TP%20Individual%20N1/carga.ipynb#W4sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m venta2\u001b[39m.\u001b[39;49mto_csv (\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mandre\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mOneDrive\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mEscritorio\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mTP Individual N1\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDS-PI-ProyectoIndividual\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdatasetsProcesados\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mventas_dataframe.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, index \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, header\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:3551\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3540\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3542\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3543\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3544\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3548\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3549\u001b[0m )\n\u001b[1;32m-> 3551\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3552\u001b[0m     path_or_buf,\n\u001b[0;32m   3553\u001b[0m     line_terminator\u001b[39m=\u001b[39;49mline_terminator,\n\u001b[0;32m   3554\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3555\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3556\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3557\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3558\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3559\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3560\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3561\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3562\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3563\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3564\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3565\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3566\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3567\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3568\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1162\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1163\u001b[0m     line_terminator\u001b[39m=\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1179\u001b[0m )\n\u001b[1;32m-> 1180\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1183\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:697\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[1;32m--> 697\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[0;32m    699\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[0;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    701\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:571\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    569\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[0;32m    570\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[1;32m--> 571\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'C:\\Users\\andre\\OneDrive\\Escritorio\\TP Individual N1\\DS-PI-ProyectoIndividual\\datasetsProcesados'"
     ]
    }
   ],
   "source": [
    "cargadeVentas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARGA Y TRATAMIENTO DE TABLA COMPRAS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargadeCompras():\n",
    "    #ds_path= pathlib.Path(\"Datasets\")\n",
    "    #print(ds_path)\n",
    "    df_list=[]\n",
    "    #archivo=glob.glob('.csv')\n",
    "    #print(\"Hola\")\n",
    "    for file in glob.glob('Compra.csv'):\n",
    "        #print(file)\n",
    "        df=pd.read_csv(file,delimiter = ',',encoding = \"utf-8\")\n",
    "        df_list.append(df)\n",
    "    compra=pd.concat(df_list)\n",
    "    Q1 = compra[\"Precio\"].quantile(0.25)\n",
    "    Q3 = compra[\"Precio\"].quantile(0.75)\n",
    "    #Se calcula el rango intercuartilico IQR.\n",
    "    IQR = Q3 -Q1\n",
    "    outliersSup = (Q3 + (1.5*IQR)) \n",
    "\n",
    "    mask = compra[\"Precio\"]<outliersSup\n",
    "    compraSinOut2 = compra[mask]\n",
    "\n",
    "    Precio_Producto= compraSinOut2.groupby([\"IdProducto\"])[\"Precio\"].mean().reset_index()\n",
    "    Precio_Producto= Precio_Producto.rename(columns= {\"Precio\":\"Precio2\"})\n",
    "\n",
    "    compra2=compra.copy()\n",
    "    compra2 = pd.merge(compra2,Precio_Producto, how=\"left\", on=[\"IdProducto\"])\n",
    "    compra2.sort_values(\"IdCompra\")\n",
    "\n",
    "    #se reemplazan los 0 de precio2 por el valor de la media de los precios sin outliers.\n",
    "    compra2.Precio2.fillna(compraSinOut2.Precio.mean(),inplace=True)\n",
    "\n",
    "    #se reemplazan los nan por 0 en la columna precio.\n",
    "    compra2.Precio.fillna(0,inplace=True)\n",
    "    #se insertan 1 cuando la cantidad es 0.\n",
    "    compra2.Cantidad.fillna(1,inplace=True)\n",
    "    #Se crea una columna con unos (1) \n",
    "    compra2[\"Outliers\"]=np.ones(compra2.shape[0])\n",
    "    #se marcan con 0 las filas que son outlaiers, para luego ser reemplazadas por Precio2\n",
    "    compra2.loc[(compra2['Precio'] >= outliersSup),\"Outliers\"] = 0\n",
    "\n",
    "    #Este bucle reemplaza los ceros y outliers por los valores promedios \n",
    "    for i in range(0,compra2.shape[0]):\n",
    "      if compra2.Precio[i] == 0 or compra2.Precio[i]>outliersSup:\n",
    "        compra2.iloc[i,7]=compra2.iloc[i,9] # 7 es precio, 9 es precio 2\n",
    "        \n",
    "    #se elimina la columna Precio2\n",
    "    compra2.drop(columns=\"Precio2\", inplace=True)\n",
    "    #Se crea la columna Total\n",
    "    #compra2[\"Total\"]= compra2[\"Precio\"]*compra2[\"Cantidad\"]\n",
    "    #Se calcula el rango intercuartilico IQR.\n",
    "    Q1 = compra2[\"Cantidad\"].quantile(0.25)\n",
    "    Q3 = compra2[\"Cantidad\"].quantile(0.75)\n",
    "    IQR = Q3 -Q1\n",
    "    outliersSup = (Q3 + (1.5*IQR)) \n",
    "    # se crea un df sin outliers, se calcula la media de compras,y se imputan como valor en los casos que sean outliers.\n",
    "    mask = compra2[\"Cantidad\"]<outliersSup\n",
    "    compraCantSinOut = compra2[mask]\n",
    "    cantidadMedia = round(compraCantSinOut.Cantidad.mean())\n",
    "    compra2.loc[(compra2['Cantidad'] >= outliersSup),\"Cantidad\"] = cantidadMedia #8\n",
    "    compra2[\"Total\"]= compra2[\"Precio\"]*compra2[\"Cantidad\"]\n",
    "    compra2.to_csv (r'C:\\Users\\andre\\OneDrive\\Escritorio\\TP Individual N1\\DS-PI-ProyectoIndividual\\datasetsProcesados\\compras_dataframe.csv', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cargadeCompras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARGA Y TRATAMIENTO DE CLIENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargadeLocalidades():\n",
    "    #ds_path= pathlib.Path(\"Datasets\")\n",
    "    #print(ds_path)\n",
    "    df_list=[]\n",
    "    #archivo=glob.glob('.csv')\n",
    "    #print(\"Hola\")\n",
    "    for file in glob.glob('Localidades.csv'):\n",
    "        #print(file)\n",
    "        df=pd.read_csv(file,delimiter = ',',encoding = \"utf-8\")\n",
    "        df_list.append(df)\n",
    "    global localidades\n",
    "    localidades=pd.concat(df_list)\n",
    "    localidades[\"nombre\"]=localidades.apply(lambda x: \"Ciudad de Buenos Aires\" if x[\"municipio_nombre\"]==\"Capital Federal\"  else x[\"nombre\"], axis=1)\n",
    "    localidades.drop(['categoria','fuente', 'localidad_censal_id', 'localidad_censal_nombre', 'municipio_id', 'municipio_nombre' ], axis = 'columns', inplace=True)\n",
    "    localidades[\"departamento_id\"].fillna(\"Sin datos\", inplace=True)\n",
    "    localidades[\"departamento_nombre\"].fillna(\"Sin datos\", inplace=True)\n",
    "    localidades= localidades.rename(columns= {\"centroide_lat\":\"Latitud\"})\n",
    "    localidades= localidades.rename(columns= {\"centroide_lon\":\"Longitud\"})\n",
    "    localidades_repetidas =localidades.id.value_counts()\n",
    "    localidades_repetidas[localidades_repetidas>1]\n",
    "    global provincias\n",
    "    #Creacion de df Provincia\n",
    "    provincias = pd.DataFrame()\n",
    "    provincias[\"Provincia\"] = localidades.provincia_nombre\n",
    "    provincias[\"IdProvincia\"]=localidades.provincia_id\n",
    "\n",
    "    provincias= provincias.drop_duplicates()\n",
    "    departamentos= pd.DataFrame()\n",
    "    departamentos[\"departamento_nombre\"] = localidades.departamento_nombre\n",
    "    departamentos[\"departamento_id\"]=localidades.departamento_id\n",
    "    departamentos= departamentos.rename(columns= {\"departamento_id\":\"IdDepartamento\",\"departamento_nombre\":\"Departamento\"})\n",
    "    departamentos= departamentos.drop_duplicates()\n",
    "    localidades.drop(columns=\"departamento_nombre\", inplace=True)\n",
    "\n",
    "\n",
    "    localidades= localidades.rename(columns= {\"provincia_nombre\":\"Provincia\"})\n",
    "    localidades[\"nombre\"]= localidades[\"nombre\"].str.title()\n",
    "    localidades[\"Provincia\"]= localidades[\"Provincia\"].str.title()\n",
    "    localidades= localidades.rename(columns= {\"id\":\"IdLocalidad\", \"nombre\":\"Localidad\",\"provincia_id\":\"IdProvincia\", \"departamento_id\":\"IdDepartamento\"})\n",
    "\n",
    "    #Borroduplicados\n",
    "    localidades.drop_duplicates(subset=[\"Localidad\",\"IdProvincia\"], inplace=True)\n",
    "    localidades.reset_index(inplace=True)\n",
    "    \n",
    "    localidades[\"Provincia\"]=[x.replace('á',\"a\") for x in localidades[\"Provincia\"]]\n",
    "    localidades[\"Provincia\"]=[x.replace('é',\"e\") for x in localidades[\"Provincia\"]]\n",
    "    localidades[\"Provincia\"]=[x.replace('í',\"i\") for x in localidades[\"Provincia\"]]\n",
    "    localidades[\"Provincia\"]=[x.replace('ó',\"o\") for x in localidades[\"Provincia\"]]\n",
    "    localidades[\"Provincia\"]=[x.replace('ú',\"u\") for x in localidades[\"Provincia\"]]\n",
    "    localidades[\"Localidad\"]=[x.replace('á',\"a\") for x in localidades[\"Localidad\"]]\n",
    "    localidades[\"Localidad\"]=[x.replace('é',\"e\") for x in localidades[\"Localidad\"]]\n",
    "    localidades[\"Localidad\"]=[x.replace('í',\"i\") for x in localidades[\"Localidad\"]]\n",
    "    localidades[\"Localidad\"]=[x.replace('ó',\"o\") for x in localidades[\"Localidad\"]]\n",
    "    localidades[\"Localidad\"]=[x.replace('ú',\"u\") for x in localidades[\"Localidad\"]]\n",
    "    \n",
    "    \n",
    "    localidades=localidades.append({'Latitud' : 0 , 'Longitud' : 0, 'IdLocalidad' : 0, 'Localidad':\"Sin Datos\",\"Provincia\":\"Sin Datos\"} , ignore_index=True)\n",
    "    localidades.to_csv (r'C:\\Users\\andre\\OneDrive\\Escritorio\\TP Individual N1\\DS-PI-ProyectoIndividual\\datasetsProcesados\\localidades_dataframe.csv', index = False, header=True)\n",
    "    provincias.to_csv (r'C:\\Users\\andre\\OneDrive\\Escritorio\\TP Individual N1\\DS-PI-ProyectoIndividual\\datasetsProcesados\\provincias_dataframe.csv', index = False, header=True)\n",
    "    departamentos.to_csv (r'C:\\Users\\andre\\OneDrive\\Escritorio\\TP Individual N1\\DS-PI-ProyectoIndividual\\datasetsProcesados\\departamentos_dataframe.csv', index = False, header=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_15040\\661035760.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  localidades=localidades.append({'Latitud' : 0 , 'Longitud' : 0, 'IdLocalidad' : 0, 'Localidad':\"Sin Datos\",\"Provincia\":\"Sin Datos\"} , ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "cargadeLocalidades()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARGA Y TRATAMIENTO DE CLIENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargadeClientes():\n",
    "    #ds_path= pathlib.Path(\"Datasets\")\n",
    "    #print(ds_path)\n",
    "    df_list=[]\n",
    "    #archivo=glob.glob('.csv')\n",
    "    #print(\"Hola\")\n",
    "    for file in glob.glob('Client*.csv'):\n",
    "        #print(file)\n",
    "        df=pd.read_csv(file,delimiter = ';',encoding = \"utf-8\", usecols=[\"ID\",\"Provincia\",\"Nombre_y_Apellido\",\"Domicilio\",\"Telefono\",\"Edad\",\"Localidad\",\"X\",\"Y\",\"col10\"])\n",
    "        df_list.append(df)\n",
    "    clientes=pd.concat(df_list)\n",
    "    clientes.drop(columns=\"col10\", inplace=True)\n",
    "    #Se corrigen nombre de Columnas \n",
    "    clientes= clientes.rename(columns= {\"ID\":\"IdCliente\"})\n",
    "    clientes= clientes.rename(columns= {\"X\":\"Longitud\"})\n",
    "    clientes= clientes.rename(columns= {\"Y\":\"Latitud\"})\n",
    "\n",
    "    #Letra capital\n",
    "    clientes[\"Nombre_y_Apellido\"]= clientes[\"Nombre_y_Apellido\"].str.title()\n",
    "    clientes[\"Domicilio\"]= clientes[\"Domicilio\"].str.title()\n",
    "    clientes[\"Localidad\"]= clientes[\"Localidad\"].str.title()\n",
    "\n",
    "    #Reemplazo de datos faltantes\n",
    "    clientes.Provincia.fillna(\"Sin Datos\", inplace=True)\n",
    "    clientes.Nombre_y_Apellido.fillna(\"Sin Datos\", inplace=True)\n",
    "    clientes.Localidad.fillna(\"Sin Datos\", inplace=True)\n",
    "\n",
    "    clientes.Telefono.fillna(\"Sin Datos\", inplace=True)\n",
    "    clientes.Domicilio.fillna(\"Sin Datos\", inplace=True)\n",
    "\n",
    "    #Reemplazo , por . \n",
    "    clientes[\"Latitud\"] = clientes[\"Latitud\"].str.replace(\",\", \".\")\n",
    "    clientes[\"Longitud\"] = clientes[\"Longitud\"].str.replace(\",\", \".\")\n",
    "\n",
    "\n",
    "    #Reemplazo Latitud y Longitud nulos  con 0\n",
    "    clientes.Latitud.fillna(0.0, inplace=True)\n",
    "    clientes.Longitud.fillna(0.0, inplace=True)\n",
    "    #Normalizo provincia de Buenos Aires \n",
    "    clientes[\"Provincia\"]=clientes.apply(lambda x: \"Buenos Aires\" if x[\"Provincia\"]==\"Ciudad de Buenos Aires\"  else x[\"Provincia\"], axis=1)\n",
    "    #Tratamiendo de acentos \n",
    "    clientes[\"Provincia\"]=[x.replace('á',\"a\") for x in clientes[\"Provincia\"]]\n",
    "    clientes[\"Provincia\"]=[x.replace('é',\"e\") for x in clientes[\"Provincia\"]]\n",
    "    clientes[\"Provincia\"]=[x.replace('í',\"i\") for x in clientes[\"Provincia\"]]\n",
    "    clientes[\"Provincia\"]=[x.replace('ó',\"o\") for x in clientes[\"Provincia\"]]\n",
    "    clientes[\"Provincia\"]=[x.replace('ú',\"u\") for x in clientes[\"Provincia\"]]\n",
    "    clientes[\"Localidad\"]=[x.replace('á',\"a\") for x in clientes[\"Localidad\"]]\n",
    "    clientes[\"Localidad\"]=[x.replace('é',\"e\") for x in clientes[\"Localidad\"]]\n",
    "    clientes[\"Localidad\"]=[x.replace('í',\"i\") for x in clientes[\"Localidad\"]]\n",
    "    clientes[\"Localidad\"]=[x.replace('ó',\"o\") for x in clientes[\"Localidad\"]]\n",
    "    clientes[\"Localidad\"]=[x.replace('ú',\"u\") for x in clientes[\"Localidad\"]]\n",
    "    #Merge entre df Localidades y Clientes para traer dato IdLocalidad a Clientes \n",
    "    clientes = pd.merge(clientes,localidades, how=\"left\", on=[\"Provincia\",\"Localidad\"])\n",
    "\n",
    "    #Reemplazo Latitud y Longitud sin datos con Latitud y Longitud de tabla Localidades \n",
    "    clientes[\"Longitud_x\"]=clientes.apply(lambda x: x[\"Longitud_y\"] if x[\"Longitud_x\"]==0  else x[\"Longitud_x\"], axis=1)\n",
    "    clientes[\"Latitud_x\"]=clientes.apply(lambda x: x[\"Latitud_y\"] if x[\"Latitud_x\"]==0  else x[\"Latitud_x\"], axis=1)\n",
    "    #Renombro columnas\n",
    "    clientes= clientes.rename(columns= {\"Longitud_x\":\"Longitud\"})\n",
    "    clientes= clientes.rename(columns= {\"Latitud_x\":\"Latitud\"})\n",
    "\n",
    "    #Cambio de tipo de dato de Latitud y Longitud\n",
    "    clientes[\"Latitud\"]= clientes.Latitud.astype(float)\n",
    "    clientes[\"Longitud\"]= clientes.Longitud.astype(float)\n",
    "    #Elimino columnas que se agregaron en el merge\n",
    "    clientes.drop(['index'], axis = 'columns', inplace=True)\n",
    "    #Funcion para encontrar Localidades por similitud - Se recupera IdLocalidad y IdProvincia\n",
    "    def verificarLocalidadClientes(row):\n",
    "        ratiomayor=0\n",
    "        for i in range(0,localidades.shape[0]): \n",
    "\n",
    "                Ratio= fuzz.token_sort_ratio(row[\"Localidad\"].lower(), localidades.Localidad[i].lower())\n",
    "                if Ratio>ratiomayor:\n",
    "                        ratiomayor=Ratio\n",
    "                        #venta2.iloc[i,8]\n",
    "                        #miloc= localidades.iloc[i,4]\n",
    "                        miloc=localidades.Localidad[i]\n",
    "                        #id=localidades.iloc[i,3]\n",
    "                        id=localidades.IdLocalidad[i]\n",
    "                        #idprov=localidades.iloc[i,5]\n",
    "                        idprov=localidades.IdProvincia[i]\n",
    "\n",
    "        row[\"Localidad\"]=miloc\n",
    "        row[\"IdLocalidad\"]=id\n",
    "        row[\"IdProvincia\"]=idprov\n",
    "\n",
    "\n",
    "        return row \n",
    "    #Creo DF de clientes con IdLocalidad faltante \n",
    "    clientes_nulos= clientes.loc[clientes['IdLocalidad'].isnull() == True].copy()\n",
    "    #Aplico funcion\n",
    "    clientes_nulos= clientes_nulos.apply(verificarLocalidadClientes,axis=1)\n",
    "    #Borro columnas innecesarias para el merge \n",
    "    clientes_nulos.drop(columns=[\"Latitud\",\"Longitud\"],inplace=True)\n",
    "\n",
    "    #Merge de Clientes con DF Clientes con IDlocalidad Nulos (ya corregido)\n",
    "    clientes = pd.merge(clientes,clientes_nulos, how=\"left\", on=[\"IdCliente\"])\n",
    "\n",
    "    #Completo IdLocalidad en tabla clientes original con 0 para prepararlos para el reemplazo\n",
    "    clientes[\"IdLocalidad_x\"]= clientes[\"IdLocalidad_x\"].fillna(0)\n",
    "    clientes[\"IdProvincia_x\"]= clientes[\"IdProvincia_x\"].fillna(0)\n",
    "    #Realizo reemplazo de IdLocalidad y IdProvincia traidos con el merge\n",
    "    clientes[\"IdLocalidad_x\"]=clientes.apply(lambda x: x[\"IdLocalidad_y\"] if x[\"IdLocalidad_x\"]==0  else x[\"IdLocalidad_x\"], axis=1)\n",
    "    clientes[\"IdProvincia_x\"]=clientes.apply(lambda x: x[\"IdProvincia_y\"] if x[\"IdProvincia_x\"]==0  else x[\"IdProvincia_x\"], axis=1)\n",
    "\n",
    "\n",
    "    #25 Localidades sin IdLocalidad - se lo completa como sin dato\n",
    "    clientes[\"IdLocalidad_x\"]=clientes[\"IdLocalidad_x\"].fillna(0)\n",
    "\n",
    "    #Elimino columnas innecesarias\n",
    "    clientes.drop(['Latitud_y_x','Longitud_y_x','IdDepartamento_x','Provincia_y','Localidad_y', 'Nombre_y_Apellido_y'], axis = 'columns', inplace=True)\n",
    "    clientes.drop(['IdProvincia_y','IdLocalidad_y','IdDepartamento_y','Longitud_y_y','Latitud_y_y', 'Edad_y', 'Telefono_y', 'Domicilio_y'], axis = 'columns', inplace=True)\n",
    "    #Renombro columnas\n",
    "    clientes= clientes.rename(columns= {\"Provincia_x\":\"Provincia\"})\n",
    "    clientes= clientes.rename(columns= {\"Nombre_y_Apellido_x\":\"Nombre_y_Apellido\"})\n",
    "    clientes= clientes.rename(columns= {\"Localidad_x\":\"Localidad\"})\n",
    "    clientes= clientes.rename(columns= {\"Longitud_x\":\"Longitud\"})\n",
    "    clientes= clientes.rename(columns= {\"Latitud_x\":\"Latitud\"})\n",
    "    clientes= clientes.rename(columns= {\"IdLocalidad_x\":\"IdLocalidad\"})\n",
    "    clientes= clientes.rename(columns= {\"IdProvincia_x\":\"IdProvincia\"})\n",
    "    clientes.to_csv (r'C:\\Users\\andre\\OneDrive\\Escritorio\\TP Individual N1\\DS-PI-ProyectoIndividual\\datasetsProcesados\\clientes_dataframe.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cargadeClientes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROVEEDORES CARGA Y TRATAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proveedores = pd.read_csv(\"Proveedores.csv\",delimiter = ',',encoding = \"ansi\")\n",
    "proveedores = proveedores.fillna(\"Sin Dato\")\n",
    "proveedores= proveedores.rename(columns= {\"IDProveedor\":\"IdProveedor\", \"Address\":\"Direccion\",\"City\":\"Ciudad\",\"State\":\"Provincia\",\"Country\":\"Pais\",\"departamen\":\"Departamento\"})\n",
    "proveedores.to_csv (r'C:\\Users\\andre\\OneDrive\\Escritorio\\TP Individual N1\\DS-PI-ProyectoIndividual\\datasetsProcesados\\proveedores_dataframe.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6847c98a8f86b01c6a19c518cd2f366693b80566b266804d5ca763cbb223f52b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
